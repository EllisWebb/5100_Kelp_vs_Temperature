import pandas as pd

excel_path = "/Users/carterwebb/Desktop/kelps/AllYearsAllSurveys_ExcelMaster_20250918_full_FOR CARTER.xlsx"

year_dfs = pd.read_excel(excel_path, sheet_name=None)

## First We Need To See How Many Beds Have Enought Data ## 
bed_years_map = {}

for yr, df in year_dfs.items(): 
    if 'Bed Name' in df.columns: #find the name column
        for bed in df['Bed Name'].dropna().unique(): #get rid of duplicates and NAN
            bed_years_map.setdefault(bed, []).append(str(yr)) #attach the bed name to the years list we created first and fill the dictionary 

print("\n | Years per Bed Name |\n")
for bed, years in sorted(bed_years_map.items()): #sort dictionary 
    years_sorted = ", ".join(sorted(years)) # sort years 
    print(f"{bed}: {years_sorted}") #print nicely 


def build_bed_dfs(excel_path, desired_cols=None):
    """
    Load the Excel file and its multiple sheets (2016–2024), keeping selected columns.
    Also creates standalone DataFrame for each target bed directly in the globals.
    """

## Select Target Beds That Have Greatest Data ## 
    target_beds = [
        'Aiston Preserve','Biz Point','Cherry Point-Gulf Rd','Clallam Bay','Coffin Rocks',
        "Ebey's Landing",'Freshwater Bay 1','Freshwater Bay 2','Hat Island','Hoypus Point',
        'Lowell','Lummi SW','North Beach East','Polnell Point','Possession Point',
        'Shannon Point East','Shannon Point West'
    ]

## Simplify Columns By Setting Default Columns## 
    if desired_cols is None:
        desired_cols = [
            'Bed Name', 'Site Code', 'Survey Date', 'Survey Day', "NWSC Max Ext",
            'Survey Month', 'Survey Year', 'Acres',
            'Temp', 'Temp1 Shore Edge', 'Temp1 Water Edge',
            'Temp2 Shore Edge', 'Temp2 Water Edge',
            'Ave Temp Shore Edge', 'Ave Temp Water Edge'
        ]

## Read Sheets As One then Concat Them Into One DF ## 
    dfs = pd.read_excel(excel_path, sheet_name=None)
    all_data = pd.concat(
        [df[[c for c in desired_cols if c in df.columns]] for df in dfs.values()],
        ignore_index=True
    )

## Standardize DF Names ## 
    for bed in target_beds:
        ## Filter Columns with Target Bed Name ##  
        mask = all_data['Bed Name'] == bed
        if mask.any():
            ## Standardize Bed Names ## 
            var_name = (
                bed.replace(" ", "_")
                   .replace("-", "_")
                   .replace("’", "")
                   .replace("'", "")
            )
## Boolean mask (True/False for each row) loading the previously selected Target Bed rows ##
            globals()[var_name] = all_data.loc[mask].copy()
            print(f"✅ Created DataFrame: {var_name} ({mask.sum()} rows)")

## First Time Using Globals ## Bassically Calls all Variables Defined at Present. 

#Simply Put Globals Now...
## globals() == {
  #  'Lowell': DataFrame,
  #  'Polnell_Point': DataFrame,
  #  'Biz_Point': DataFrame,
  #  ...
  #  'latlon_clean': DataFrame,
  #  'pd': Library          
  #  ...
# }


## Load Path and Use Function ## 
bed_dfs = build_bed_dfs(excel_path)


Hoypus_Point


## Looking Into Each Data Frame For Info ##
for var_name in [
    'Aiston_Preserve','Biz_Point','Cherry_Point_Gulf_Rd','Clallam_Bay','Coffin_Rocks',
    'Ebeys_Landing','Freshwater_Bay_1','Freshwater_Bay_2','Hat_Island','Hoypus_Point',
    'Lowell','Lummi_SW','North_Beach_East','Polnell_Point','Possession_Point',
    'Shannon_Point_East','Shannon_Point_West'
]:
    print(f"\n | {var_name} |\n")
    globals()[var_name].info()


# Save Lat Lon Incase We Need it In the Future ## 
latlon_clean.to_csv("/Users/carterwebb/Desktop/kelps/data/cleaned_data/latlon_clean.csv", index=False)
print("✅ Saved latlon_clean.csv")

## Couldnt Get the Old List To Call Correctly, Not Sure What Happened, Simply Remade it To Save ## 
bed_vars = [
    'Aiston_Preserve','Biz_Point','Cherry_Point_Gulf_Rd','Clallam_Bay','Coffin_Rocks',
    'Ebeys_Landing','Freshwater_Bay_1','Freshwater_Bay_2','Hat_Island','Hoypus_Point',
    'Lowell','Lummi_SW','North_Beach_East','Polnell_Point','Possession_Point',
    'Shannon_Point_East','Shannon_Point_West'
]

for bed in bed_vars:
    if bed in globals():
        df = globals()[bed]
        if 'Bed Name' not in df.columns:
            df['Bed Name'] = bed
        df.to_csv(f"/Users/carterwebb/Desktop/kelps/data/cleaned_data/{bed}.csv", index=False)
        print(f"✅ Saved {bed}.csv ({len(df)} rows)")
    else:
        print(f"⚠️ {bed} not found")

## Concat For Future Use If Neccesary / Making Percent Change Metrics ## 
combined = pd.concat([globals()[b] for b in bed_vars if b in globals()], ignore_index=True)
combined.to_csv("/Users/carterwebb/Desktop/kelps/data/cleaned_data/AllBeds_Clean.csv", index=False)
print(f"✅ Saved AllBeds_Clean.csv ({len(combined)} total rows)")



